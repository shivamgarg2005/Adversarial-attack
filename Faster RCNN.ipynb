{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ebc6b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tarfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f921d2932261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLopener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDOWNLOAD_BASE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODEL_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtar_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtar_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmembers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tarfile' is not defined"
     ]
    }
   ],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13c625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import zipfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import six.moves.urllib as urllib\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils.visualization_utils import visualize_boxes_and_labels_on_image_array\n",
    "import PIL.Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eefbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'faster_rcnn_inception_v2_coco_2017_11_08'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = 'data/mscoco_label_map.pbtxt'\n",
    "NUM_CLASSES = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e32d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db1607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2arr(path):\n",
    "    im = cv2.imread(path)\n",
    "    img = cv2.cvtColor(cv2.resize(im, (600, 600)), cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def arr2img(a, fmt='png'):\n",
    "    cv2.imshow(\"Color Image\",a[:, :, ::-1])\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e180b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "psize = 600\n",
    "inference_graph = tf.Graph()\n",
    "with inference_graph.as_default():\n",
    "    image_tensor = tf.placeholder(tf.float32, shape=(None, psize, psize, 3), name='image_tensor')\n",
    "    inference_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        inference_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(inference_graph_def, name='',\n",
    "                            input_map={'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3:0':image_tensor})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad18fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detections(img, scores=None, bboxes=None, min_threshold=0):\n",
    "    if scores is None or bboxes is None:\n",
    "        inference_sess = tf.Session(graph=inference_graph)\n",
    "\n",
    "        tensors = [ inference_graph.get_tensor_by_name('num_detections:0'),\n",
    "                    inference_graph.get_tensor_by_name('SecondStagePostprocessor/Reshape_4:0'),\n",
    "                    inference_graph.get_tensor_by_name('SecondStagePostprocessor/convert_scores:0') ]\n",
    "\n",
    "        feed_dict = { inference_graph.get_tensor_by_name('image_tensor:0'): np.expand_dims(img, axis=0) }\n",
    "\n",
    "        num_detections, bboxes, scores = inference_sess.run(tensors, feed_dict)\n",
    "        bboxes = bboxes[0]\n",
    "        scores = scores[0]\n",
    "        \n",
    "    sorted_classes = np.argsort(scores[:, 1:], axis=1)\n",
    "    sorted_scores = scores[:, 1:].copy()\n",
    "    sorted_bboxes = bboxes.copy()\n",
    "\n",
    "    for i, ordering in enumerate(sorted_classes):\n",
    "        sorted_scores[i, :] = scores[i, ordering+1]\n",
    "        sorted_bboxes[i, :] = bboxes[i, ordering, :]\n",
    "\n",
    "    sorted_classes += 1\n",
    "\n",
    "    img_viz = visualize_boxes_and_labels_on_image_array(img.copy(), \n",
    "                                                        sorted_bboxes[:, -1, :],\n",
    "                                                        sorted_classes[:, -1].astype(np.int32),\n",
    "                                                        sorted_scores[:, -1],\n",
    "                                                        category_index,\n",
    "                                                        use_normalized_coordinates=False,\n",
    "                                                        max_boxes_to_draw=sorted_scores.shape[1],\n",
    "                                                        min_score_thresh=min_threshold,\n",
    "                                                        line_thickness=2)\n",
    "    arr2img(img_viz)\n",
    "\n",
    "img = img2arr('data/img_stop_sign.png')\n",
    "plot_detections(img, min_threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e1beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "psize = 600\n",
    "RED_MASK_NAME = 'data/stop_red_mask.npy'\n",
    "RED_MASK = np.array(PIL.Image.fromarray(np.load(RED_MASK_NAME)).resize((psize, psize)))\n",
    "arr2img(255*RED_MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b27a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITE_MASK_NAME = 'data/stop_white_mask.npy'\n",
    "WHITE_MASK = np.array(PIL.Image.fromarray(np.load(WHITE_MASK_NAME)).resize((psize, psize)))\n",
    "arr2img(255*WHITE_MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85e67127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-e6ef6b7b06c2>:98: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-e6ef6b7b06c2>:98: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-e6ef6b7b06c2>:133: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-e6ef6b7b06c2>:133: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/Desktop/attacks/env/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/Desktop/attacks/env/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading the model, took 4s\n"
     ]
    }
   ],
   "source": [
    "class ModelContainer():\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.patch_shape = (psize, psize, 3)\n",
    "        self.batch_size_ = 10\n",
    "        self._make_model_and_ops(None)\n",
    "\n",
    "    def get_patch(self):\n",
    "        patch = np.round((self._run(self.clipped_patch_)+1)*(255/2.0)).astype(np.uint8)\n",
    "        patch *= RED_MASK\n",
    "        patch[patch == 0] = 255\n",
    "        return patch\n",
    "\n",
    "    def assign_patch(self, new_patch):\n",
    "        self._run(self.assign_patch_, {self.patch_placeholder_: new_patch})\n",
    "\n",
    "    def reset_patch(self):\n",
    "        self.assign_patch(np.zeros(self.patch_shape))\n",
    "          \n",
    "    def train_step(self, images, patch_transforms, second_stage_cls_labels, learning_rate=1.0,\n",
    "                   dropout=None, rpn_nms_bboxes=None, rpn_nms_indices=None, patch_loss_weight=None):\n",
    "        if (rpn_nms_bboxes is None) or \\\n",
    "           (rpn_nms_indices is None):\n",
    "            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n",
    "\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms,\n",
    "                      self.second_stage_cls_labels_: second_stage_cls_labels,\n",
    "                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n",
    "                      self.rpn_nms_indices_placeholder_: rpn_nms_indices,\n",
    "                      self.learning_rate_: learning_rate }\n",
    "        \n",
    "        if patch_loss_weight is not None:\n",
    "            feed_dict[self.patch_loss_weight_] = patch_loss_weight\n",
    "        \n",
    "        tensors = [ self.train_op_,\n",
    "                    self.loss_,\n",
    "                    self.second_stage_cls_loss_, \n",
    "                    self.patch_loss_]\n",
    "\n",
    "        train_op, loss, second_stage_cls_loss, patch_loss = self._run(tensors, feed_dict, dropout=dropout)\n",
    "\n",
    "        return loss, second_stage_cls_loss, patch_loss\n",
    "    \n",
    "    def inference_rpn(self, images, patch_transforms):\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms }\n",
    "        \n",
    "        tensors = [self.rpn_nms_bboxes_,\n",
    "                   self.rpn_nms_indices_ ]\n",
    "\n",
    "        rpn_nms_bboxes, rpn_nms_indices = self._run(tensors, feed_dict)\n",
    "        \n",
    "        return rpn_nms_bboxes, rpn_nms_indices\n",
    "\n",
    "    def inference(self, images, patch_transforms, rpn_nms_bboxes=None, rpn_nms_indices=None):\n",
    "        if (rpn_nms_bboxes is None) or \\\n",
    "           (rpn_nms_indices is None):\n",
    "            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n",
    "\n",
    "        feed_dict = { self.image_input_: images,\n",
    "                      self.patch_transforms_: patch_transforms,\n",
    "                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n",
    "                      self.rpn_nms_indices_placeholder_: rpn_nms_indices }\n",
    "        second_stage_cls_scores = np.random.rand(1,300,91)\n",
    "        tensors = [ self.patched_input_,\n",
    "                    self.second_stage_cls_scores_,\n",
    "                    self.second_stage_loc_bboxes_ ]\n",
    "        \n",
    "        patched_imgs, second_stage_loc_bboxes = self.inference_rpn(images, patch_transforms)\n",
    "        \n",
    "        return patched_imgs, second_stage_cls_scores, second_stage_loc_bboxes\n",
    "\n",
    "    def _run(self, target, feed_dict=None, dropout=None):\n",
    "        if feed_dict is None:\n",
    "            feed_dict = {}\n",
    "         \n",
    "        if dropout is not None:\n",
    "            feed_dict[self.dropout_] = dropout\n",
    "        return self.sess.run(target, feed_dict=feed_dict)\n",
    "    \n",
    "    def _make_model_and_ops(self, patch_val):\n",
    "        start = time.time()\n",
    "        with self.sess.graph.as_default():\n",
    "            tf.set_random_seed(1234)\n",
    "            \n",
    "            # Tensors are post-fixed with an underscore!\n",
    "            self.image_input_ = tf.placeholder(tf.float32, shape=(None, psize, psize, 3), name='image_input')\n",
    "            self.patch_transforms_ = tf.placeholder(tf.float32, shape=(None, 8), name='patch_transforms')\n",
    "\n",
    "            patch_ = tf.get_variable('patch', self.patch_shape, dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "            self.patch_placeholder_ = tf.placeholder(dtype=tf.float32, shape=self.patch_shape, name='patch_placeholder')\n",
    "            self.assign_patch_ = tf.assign(patch_, self.patch_placeholder_)\n",
    "            self.clipped_patch_ = tf.tanh(patch_)\n",
    "\n",
    "            self.dropout_ = tf.placeholder_with_default(1.0, [], name='dropout')\n",
    "            patch_with_dropout_ = tf.nn.dropout(self.clipped_patch_, keep_prob=self.dropout_)\n",
    "            patched_input_ = tf.clip_by_value(self._random_overlay(self.image_input_, patch_with_dropout_), clip_value_min=-1.0, clip_value_max=1.0)\n",
    "            patched_input_ = tf.clip_by_value(tf.image.random_brightness(patched_input_, 10.0/255), -1.0, 1.0)\n",
    "            self.patched_input_ = tf.fake_quant_with_min_max_vars((patched_input_ + 1)*127.5, min=0, max=255)\n",
    "\n",
    "            # Create placeholders for NMS RPN inputs\n",
    "            self.rpn_nms_bboxes_placeholder_ = tf.placeholder(tf.float32, shape=(None, 4), name='rpn_nms_bboxes')\n",
    "            self.rpn_nms_indices_placeholder_ = tf.placeholder(tf.int32, shape=(None), name='rpn_nms_indices')\n",
    "\n",
    "            detection_graph_def = tf.GraphDef()\n",
    "            with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "                serialized_graph = fid.read()\n",
    "                detection_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(detection_graph_def, name='detection',\n",
    "                                    input_map={\n",
    "                                               'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3:0':self.patched_input_,\n",
    "                                               'Reshape_7:0':self.rpn_nms_bboxes_placeholder_,\n",
    "                                               'Reshape_8:0':self.rpn_nms_indices_placeholder_,\n",
    "                                              })\n",
    "\n",
    "            # Recreate tensors we just replaced in the input_map\n",
    "            self.rpn_nms_bboxes_ = tf.reshape(self.graph.get_tensor_by_name('detection/Reshape_6:0'), self.graph.get_tensor_by_name('detection/stack_3:0'), name='detection/Reshape_7')\n",
    "            self.rpn_nms_indices_ = tf.reshape(self.graph.get_tensor_by_name('detection/mul_1:0'), self.graph.get_tensor_by_name('detection/Reshape_8/shape:0'), name='detection/Reshape_8') \n",
    "\n",
    "            # Patch Loss\n",
    "            self.patch_loss_ = tf.nn.l2_loss(RED_MASK*(self.clipped_patch_ - np.tile(np.array([ 1.0, -0.9, -1]), (psize, psize, 1))))\n",
    "            self.patch_loss_weight_ = tf.placeholder_with_default(1.0, [], 'patch_loss_weight')\n",
    "\n",
    "            # Second-stage Class Loss\n",
    "            self.second_stage_cls_scores_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/convert_scores:0')\n",
    "            second_stage_cls_logits_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/scale_logits:0')\n",
    "            self.second_stage_cls_labels_ = tf.placeholder(tf.float32, shape=second_stage_cls_logits_.shape, name='second_stage_cls_labels')\n",
    "            second_stage_cls_losses_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(self.second_stage_cls_labels_, (-1, self.second_stage_cls_labels_.shape[2])),\n",
    "                                                                                      logits=tf.reshape(second_stage_cls_logits_, (-1, second_stage_cls_logits_.shape[2]))) \n",
    "            second_stage_cls_losses_ = tf.reshape(second_stage_cls_losses_, (-1, self.second_stage_cls_labels_.shape[1]))\n",
    "            second_stage_cls_losses_ = tf.divide(second_stage_cls_losses_, tf.to_float(self.second_stage_cls_labels_.shape[1]))\n",
    "            self.second_stage_cls_loss_ = tf.reduce_sum(second_stage_cls_losses_)\n",
    "           \n",
    "            # Second-stage bounding boxes\n",
    "            self.second_stage_loc_bboxes_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/Reshape_4:0')\n",
    "    \n",
    "            # Sum of weighted losses\n",
    "            self.loss_ = self.patch_loss_*self.patch_loss_weight_ + (self.second_stage_cls_loss_)\n",
    "\n",
    "            # Train our attack by only training on the patch variable\n",
    "            self.learning_rate_ = tf.placeholder(tf.float32)\n",
    "            self.train_op_ = tf.train.GradientDescentOptimizer(self.learning_rate_).minimize(self.loss_, var_list=[patch_])\n",
    "            \n",
    "            if patch_val is not None:\n",
    "                self.assign_patch(patch_val)\n",
    "            else:\n",
    "                self.reset_patch()\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Finished loading the model, took {:.0f}s\".format(elapsed))\n",
    "    \n",
    "\n",
    "    def _random_overlay(self, imgs, patch):    \n",
    "        red_mask = RED_MASK.astype(np.float32)\n",
    "        white_mask = WHITE_MASK.astype(np.float32)\n",
    "        \n",
    "        red_mask = tf.stack([red_mask] * self.batch_size_)\n",
    "        white_mask = tf.stack([white_mask] * self.batch_size_)\n",
    "        padded_patch = tf.stack([patch] * self.batch_size_)\n",
    "        \n",
    "        white = tf.ones_like(red_mask) * 0.95\n",
    "              \n",
    "        red_mask = tf.contrib.image.transform(red_mask, self.patch_transforms_, 'BILINEAR')\n",
    "        white_mask = tf.contrib.image.transform(white_mask, self.patch_transforms_, 'BILINEAR')\n",
    "        padded_patch = tf.contrib.image.transform(padded_patch, self.patch_transforms_, 'BILINEAR')\n",
    "\n",
    "        inverted_mask = (1 - red_mask - white_mask)\n",
    "\n",
    "        return white * white_mask + imgs * inverted_mask + padded_patch * red_mask\n",
    "    \n",
    "\n",
    "    def _transform_vector(self, width, x_shift, y_shift, im_scale, rot_in_degrees):\n",
    "        \"\"\"\n",
    "        If one row of transforms is [a0, a1, a2, b0, b1, b2, c0, c1], \n",
    "        then it maps the output point (x, y) to a transformed input point \n",
    "        (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k), \n",
    "        where k = c0 x + c1 y + 1. \n",
    "        The transforms are inverted compared to the transform mapping input points to output points.\n",
    "        \"\"\"\n",
    "\n",
    "        rot = float(rot_in_degrees) / 90. * (math.pi/2)\n",
    "\n",
    "        # Standard rotation matrix\n",
    "        # (use negative rot because tf.contrib.image.transform will do the inverse)\n",
    "        rot_matrix = np.array(\n",
    "            [[math.cos(-rot), -math.sin(-rot)],\n",
    "            [math.sin(-rot), math.cos(-rot)]]\n",
    "        )\n",
    "\n",
    "        # Scale it\n",
    "        # (use inverse scale because tf.contrib.image.transform will do the inverse)\n",
    "        inv_scale = 1. / im_scale \n",
    "        xform_matrix = rot_matrix * inv_scale\n",
    "        a0, a1 = xform_matrix[0]\n",
    "        b0, b1 = xform_matrix[1]\n",
    "\n",
    "        # At this point, the image will have been rotated around the top left corner,\n",
    "        # rather than around the center of the image. \n",
    "        #\n",
    "        # To fix this, we will see where the center of the image got sent by our transform,\n",
    "        # and then undo that as part of the translation we apply.\n",
    "        x_origin = float(width) / 2\n",
    "        y_origin = float(width) / 2\n",
    "\n",
    "        x_origin_shifted, y_origin_shifted = np.matmul(\n",
    "            xform_matrix,\n",
    "            np.array([x_origin, y_origin]),\n",
    "        )\n",
    "\n",
    "        x_origin_delta = x_origin - x_origin_shifted\n",
    "        y_origin_delta = y_origin - y_origin_shifted\n",
    "\n",
    "        # Combine our desired shifts with the rotation-induced undesirable shift\n",
    "        a2 = x_origin_delta - (x_shift/(2*im_scale))\n",
    "        b2 = y_origin_delta - (y_shift/(2*im_scale))\n",
    "\n",
    "        # Return these values in the order that tf.contrib.image.transform expects\n",
    "        return np.array([a0, a1, a2, b0, b1, b2, 0, 0]).astype(np.float32)\n",
    "\n",
    "    def generate_random_transformation(self, scale_min=0.2, scale_max=0.6, width=psize, max_rotation=20):\n",
    "        im_scale = np.random.uniform(low=scale_min, high=scale_max)\n",
    "\n",
    "        padding_after_scaling = (1-im_scale) * width\n",
    "        x_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n",
    "        y_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n",
    "\n",
    "        rot = np.random.uniform(-max_rotation, max_rotation)\n",
    "\n",
    "        return self._transform_vector(width, \n",
    "                                      x_shift=x_delta,\n",
    "                                      y_shift=y_delta,\n",
    "                                      im_scale=im_scale, \n",
    "                                      rot_in_degrees=rot)    \n",
    "\n",
    "model = ModelContainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f908f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_labels(scores, from_class, to_class):\n",
    "    target_labels = np.zeros_like(scores)\n",
    "    classes = np.argmax(scores[:, :, 1:], axis=2)+1\n",
    "\n",
    "    for i, _ in enumerate(classes):\n",
    "        for j, cls in enumerate(classes[i]):\n",
    "            cls = to_class # Just perturb all of them!\n",
    "            target_labels[i, j, cls] = 1\n",
    "\n",
    "    return target_labels\n",
    "\n",
    "# use half white images and half random noise images as the background images for the optimization\n",
    "white_imgs = np.ones([int(model.batch_size_ / 2), psize, psize, 3])\n",
    "noisy_imgs = np.random.rand(int(model.batch_size_ / 2), psize, psize, 3) * 2 - 1.0\n",
    "bg_imgs = np.concatenate([ noisy_imgs, white_imgs])\n",
    "patch_transformations = np.zeros((model.batch_size_, 8))\n",
    "for i in range(patch_transformations.shape[0]):\n",
    "    patch_transformations[i, :] = model.generate_random_transformation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7deec81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "patch_loss_weight = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6207c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e41c027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CLASS = 1\n",
    "_, scores, _ = model.inference(bg_imgs, patch_transformations)\n",
    "target_labels = create_target_labels(scores, 13, TARGET_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(251):\n",
    "    try:\n",
    "        # Generate random transformations\n",
    "        for j in range(patch_transformations.shape[0]):\n",
    "            patch_transformations[j, :] = model.generate_random_transformation(scale_min=0.05, scale_max=0.3)\n",
    "\n",
    "        # Update patch according to changed labels\n",
    "        loss, second_stage_cls_loss, patch_loss = model.train_step(bg_imgs,\n",
    "                                                                   patch_transformations,\n",
    "                                                                   target_labels,\n",
    "                                                                   learning_rate=learning_rate,\n",
    "                                                                   patch_loss_weight=patch_loss_weight)\n",
    "\n",
    "        if (i % 50) == 0:\n",
    "            print('iter {} total loss: {} target loss: {} patch loss: {}'.format(i, loss, second_stage_cls_loss, patch_loss))\n",
    "            model.inference(bg_imgs, patch_transformations)\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('iter {} total loss: {} target loss: {} patch loss: {}'.format(i, loss, second_stage_cls_loss, patch_loss))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = model.get_patch()\n",
    "showarray(PIL.Image.fromarray(patch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32104b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = model.get_patch()\n",
    "for j in range(patch_transformations.shape[0]):\n",
    "    patch_transformations[j, :] = model.generate_random_transformation(scale_min=0.05, scale_max=0.3)\n",
    "_ = model.inference(bg_imgs, patch_transformations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
